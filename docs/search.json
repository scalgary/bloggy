[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Learn Every Day",
    "section": "",
    "text": "Welcome To My Blog\n\n\n\n\n\n\nmanual\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nISLP Introduction\n\n\n\n\n\n\nlearning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to use Github actions to build a docker image\n\n\n\n\n\n\nGit\n\n\nDocker\n\n\n\n\n\n\n\n\n\nMar 18, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Using the Welcome Post as my own manual for managing this blog. Welcome!\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Setting Up a GitHub Repository with Docker and GitHub Actions",
    "section": "",
    "text": "Quick manual on how to use Github actions to build a docker image"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bonjour",
    "section": "",
    "text": "Hi! I’m Sandrine. This is my personal website. Here you can find a lot of content about Machine Learning, R, Python, Git and GitHub and anything I’m interested in. Feel free to explore my website and get in touch if you have any suggestion!"
  },
  {
    "objectID": "posts/new_post/index.html",
    "href": "posts/new_post/index.html",
    "title": "How to add a new Post",
    "section": "",
    "text": "It is very easy.\n\nCreate a sub-directory within posts and add an index.qmd file in this directory\n\nThat qmd file is the new blog post and when you render that, the blog home page will automatically update to include the newest post at the top of the listing.\nThen just do quarto preview."
  },
  {
    "objectID": "posts/post-with-code/index.html#example-files",
    "href": "posts/post-with-code/index.html#example-files",
    "title": "Setting Up a GitHub Repository with Docker and GitHub Actions",
    "section": "Example Files",
    "text": "Example Files\n\napp.py\nprint(\"Hello Test!\")\n\n\nrequirements.txt\n# This file can be left empty or include dependencies\n\n\n.gitignore\n# This file can be left empty or include dependencies"
  },
  {
    "objectID": "posts/post-with-code/index.html#dockerfile",
    "href": "posts/post-with-code/index.html#dockerfile",
    "title": "Setting Up a GitHub Repository with Docker and GitHub Actions",
    "section": "Dockerfile",
    "text": "Dockerfile\n# Use an official Python image as the base\nFROM python:3.11\n\n# Set the working directory in the container\nWORKDIR /app\n\n# Copy project files into the container\nCOPY . /app/\n\n# Create a virtual environment in /app/venv\nRUN python -m venv venv\n\n# Activate the virtual environment and install dependencies\nRUN /app/venv/bin/pip install --upgrade pip && \\\n    /app/venv/bin/pip install --no-cache-dir -r requirements.txt\n\n# Run the application using the virtual environment\nCMD [\"/app/venv/bin/python\", \"app.py\"]"
  },
  {
    "objectID": "posts/post-with-code/index.html#github-actions-workflow-docker-build.yml",
    "href": "posts/post-with-code/index.html#github-actions-workflow-docker-build.yml",
    "title": "Setting Up a GitHub Repository with Docker and GitHub Actions",
    "section": "GitHub Actions Workflow (docker-build.yml)",
    "text": "GitHub Actions Workflow (docker-build.yml)\nname: Build and Push Docker Image\n\non:\n  push:\n    branches:\n      - main  # Change this if using a different branch\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v3\n\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v2\n\n      - name: Log in to DockerHub\n        uses: docker/login-action@v2\n        with:\n          username: ${{ secrets.DOCKER_USERNAME }}\n          password: ${{ secrets.DOCKER_PASSWORD }}\n\n      - name: Build and Push Docker Image\n        uses: docker/build-push-action@v4\n        with:\n          context: .\n          file: ./Dockerfile\n          push: true\n          platforms: linux/amd64,linux/arm64\n          tags: ${{ secrets.DOCKER_USERNAME }}/docker_test:latest"
  },
  {
    "objectID": "posts/post-with-code/index.html#storing-docker-hub-credentials-in-github-secrets",
    "href": "posts/post-with-code/index.html#storing-docker-hub-credentials-in-github-secrets",
    "title": "Setting Up a GitHub Repository with Docker and GitHub Actions",
    "section": "Storing Docker Hub Credentials in GitHub Secrets",
    "text": "Storing Docker Hub Credentials in GitHub Secrets\nSince a free DockerHub account allows only one private repository, credentials must be stored securely in GitHub:\n\nGo to GitHub &gt; Your Repository &gt; Settings &gt; Secrets and variables &gt; Actions.\nClick New repository secret and add:\n\nDOCKER_USERNAME = Your DockerHub username.\nDOCKER_PASSWORD = Your DockerHub password."
  },
  {
    "objectID": "posts/post-with-code/index.html#verifying-the-setup",
    "href": "posts/post-with-code/index.html#verifying-the-setup",
    "title": "Setting Up a GitHub Repository with Docker and GitHub Actions",
    "section": "Verifying the Setup",
    "text": "Verifying the Setup\nAfter pushing changes to the repository, GitHub will automatically build and push the Docker image to DockerHub. You can verify the process:\n\nOn DockerHub: Check that the image was successfully pushed.\nOn GitHub: Inspect the logs in GitHub Actions to confirm the build and push process was completed successfully."
  },
  {
    "objectID": "posts/post-with-code/index.html#setting-up-a-git-repository",
    "href": "posts/post-with-code/index.html#setting-up-a-git-repository",
    "title": "Setting Up a GitHub Repository with Docker and GitHub Actions",
    "section": "Setting Up a Git Repository",
    "text": "Setting Up a Git Repository\nTo set up a Git repository and integrate it with Docker and GitHub Actions:\n\nCreate a new repository on GitHub.\nClone the repository to your local machine.\nEnsure the following minimum required files are included in the repository:\n\n.github/workflows/docker-build.yml: Defines the GitHub Actions workflow for building and pushing the Docker image.\nDockerfile: Contains the instructions to build the Docker image.\napp.py: A simple Python script (e.g., print(\"Hello Test!\")).\nrequirements.txt: Can be empty or contain dependencies.\n.gitignore: Can be empty but is useful for ignoring unnecessary files.\n\n\n\nExample Files\n\napp.py\nprint(\"Hello Test!\")\n\n\nrequirements.txt\n# This file can be left empty or include dependencies\n\n\n.gitignore\n# This file can be left empty or include dependencies\n\n\nDockerfile\n# Use an official Python image as the base\nFROM python:3.11\n\n# Set the working directory in the container\nWORKDIR /app\n\n# Copy project files into the container\nCOPY . /app/\n\n# Create a virtual environment in /app/venv\nRUN python -m venv venv\n\n# Activate the virtual environment and install dependencies\nRUN /app/venv/bin/pip install --upgrade pip && \\\n    /app/venv/bin/pip install --no-cache-dir -r requirements.txt\n\n# Run the application using the virtual environment\nCMD [\"/app/venv/bin/python\", \"app.py\"]\n\n\ndocker-build.yml\nname: Build and Push Docker Image\n\non:\n  push:\n    branches:\n      - main  # Change this if using a different branch\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v3\n\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v2\n\n      - name: Log in to DockerHub\n        uses: docker/login-action@v2\n        with:\n          username: ${{ secrets.DOCKER_USERNAME }}\n          password: ${{ secrets.DOCKER_PASSWORD }}\n\n      - name: Build and Push Docker Image\n        uses: docker/build-push-action@v4\n        with:\n          context: .\n          file: ./Dockerfile\n          push: true\n          platforms: linux/amd64,linux/arm64\n          tags: ${{ secrets.DOCKER_USERNAME }}/docker_test:latest"
  },
  {
    "objectID": "posts/github-docker/index.html",
    "href": "posts/github-docker/index.html",
    "title": "How to use Github actions to build a docker image",
    "section": "",
    "text": "Setting Up a GitHub Repository with Docker and GitHub Actions"
  },
  {
    "objectID": "posts/github-docker/index.html#setting-up-a-git-repository",
    "href": "posts/github-docker/index.html#setting-up-a-git-repository",
    "title": "How to use Github actions to build a docker image",
    "section": "Setting Up a Git Repository",
    "text": "Setting Up a Git Repository\n\nCreate a new repository on GitHub.\nClone the repository to your local machine.\nEnsure the following minimum required files are included in the repository:\n\n.github/workflows/docker-build.yml: Defines the GitHub Actions workflow for building and pushing the Docker image.\nDockerfile: Contains the instructions to build the Docker image.\napp.py: A simple Python script (e.g., print(\"Hello Test!\")).\nrequirements.txt: Can be empty or contain dependencies.\n.gitignore: Can be empty but is useful for ignoring unnecessary files."
  },
  {
    "objectID": "posts/github-docker/index.html#storing-docker-hub-credentials-in-github-secrets",
    "href": "posts/github-docker/index.html#storing-docker-hub-credentials-in-github-secrets",
    "title": "How to use Github actions to build a docker image",
    "section": "Storing Docker Hub Credentials in GitHub Secrets",
    "text": "Storing Docker Hub Credentials in GitHub Secrets\nCredentials must be stored securely in GitHub:\n\nGo to GitHub &gt; Your Repository &gt; Settings &gt; Secrets and variables &gt; Actions.\nClick New repository secret and add:\n\nDOCKER_USERNAME = Your DockerHub username.\nDOCKER_PASSWORD = Your DockerHub password.\n\n\nA free DockerHub account allows only one private repository."
  },
  {
    "objectID": "posts/github-docker/index.html#verifying-the-setup",
    "href": "posts/github-docker/index.html#verifying-the-setup",
    "title": "How to use Github actions to build a docker image",
    "section": "Verifying the Setup",
    "text": "Verifying the Setup\nAfter pushing changes to the repository, GitHub will automatically build and push the Docker image to DockerHub. You can verify the process:\n\nOn DockerHub: Check that the image was successfully pushed.\nOn GitHub: Inspect the logs in GitHub Actions to confirm the build and push process was completed successfully."
  },
  {
    "objectID": "posts/welcome/index.html#test2",
    "href": "posts/welcome/index.html#test2",
    "title": "Welcome To My Blog",
    "section": "test2",
    "text": "test2"
  },
  {
    "objectID": "posts/github-docker/index.html#creating-some-files",
    "href": "posts/github-docker/index.html#creating-some-files",
    "title": "How to use Github actions to build a docker image",
    "section": "Creating some files",
    "text": "Creating some files\n\napp.py\n\nprint(\"Hello Test!\")\n\nrequirements.txt\n\n# This file can be left empty or include dependencies\n\n.gitignore\n\n# This file can be left empty or include dependencies\n\nDockerfile\n\n# Use an official Python image as the base\nFROM python:3.11\n\n# Set the working directory in the container\nWORKDIR /app\n\n# Copy project files into the container\nCOPY . /app/\n\n# Create a virtual environment in /app/venv\nRUN python -m venv venv\n\n# Activate the virtual environment and install dependencies\nRUN /app/venv/bin/pip install --upgrade pip && \\\n    /app/venv/bin/pip install --no-cache-dir -r requirements.txt\n\n# Run the application using the virtual environment\nCMD [\"/app/venv/bin/python\", \"app.py\"]\n\ndocker-build.yml\n\nname: Build and Push Docker Image\n\non:\n  push:\n    branches:\n      - main  # Change this if using a different branch\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v3\n\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v2\n\n      - name: Log in to DockerHub\n        uses: docker/login-action@v2\n        with:\n          username: ${{ secrets.DOCKER_USERNAME }}\n          password: ${{ secrets.DOCKER_PASSWORD }}\n\n      - name: Build and Push Docker Image\n        uses: docker/build-push-action@v4\n        with:\n          context: .\n          file: ./Dockerfile\n          push: true\n          platforms: linux/amd64,linux/arm64\n          tags: ${{ secrets.DOCKER_USERNAME }}/docker_test:latest\nNote: I set up platforms as I will use the image on a Mac."
  },
  {
    "objectID": "About.html",
    "href": "About.html",
    "title": "Bonjour",
    "section": "",
    "text": "Hi! I’m Sandrine. This is my personal website. Here you can find a lot of content about Machine Learning, R, Python, Git and GitHub and anything I’m interested in. Feel free to explore my website and get in touch if you have any suggestion!"
  },
  {
    "objectID": "posts/welcome/index.html#how-to-add-a-new-post",
    "href": "posts/welcome/index.html#how-to-add-a-new-post",
    "title": "Welcome To My Blog",
    "section": "How to add a new post",
    "text": "How to add a new post\nIt is very easy.\n\nCreate a sub-directory within posts and add an index.qmd file in this directory\n\nThat qmd file is the new blog post and when you render that, the blog home page will automatically update to include the newest post at the top of the listing.\nThen just do quarto preview."
  },
  {
    "objectID": "posts/welcome/index.html#how-to",
    "href": "posts/welcome/index.html#how-to",
    "title": "Welcome To My Blog",
    "section": "How to",
    "text": "How to"
  },
  {
    "objectID": "posts/welcome/index.html#how-to-update",
    "href": "posts/welcome/index.html#how-to-update",
    "title": "Welcome To My Blog",
    "section": "How to update",
    "text": "How to update"
  },
  {
    "objectID": "posts/welcome/index.html#how-to-update-css",
    "href": "posts/welcome/index.html#how-to-update-css",
    "title": "Welcome To My Blog",
    "section": "How to update css",
    "text": "How to update css\nThe easiest way is to open the website in Chrome and look inspect the page. Then when you find exactly the class you want to update, you just update the scss file\nFor example, I wanted the TOC title in blue: I have added\nh2#toc-title {\n  color:$blue;\n}"
  },
  {
    "objectID": "posts/welcome/index.html#using-the-welcome",
    "href": "posts/welcome/index.html#using-the-welcome",
    "title": "Welcome To My Blog",
    "section": "Using the Welcome",
    "text": "Using the Welcome"
  },
  {
    "objectID": "posts/welcome/index.html#using-the-welcome-post-as-a",
    "href": "posts/welcome/index.html#using-the-welcome-post-as-a",
    "title": "Welcome To My Blog",
    "section": "Using the Welcome Post as a",
    "text": "Using the Welcome Post as a"
  },
  {
    "objectID": "posts/welcome/index.html#using-the-welcome-post-as-my-ow",
    "href": "posts/welcome/index.html#using-the-welcome-post-as-my-ow",
    "title": "Welcome To My Blog",
    "section": "Using the Welcome Post as my ow",
    "text": "Using the Welcome Post as my ow"
  },
  {
    "objectID": "posts/welcome/index.html#using-the-welcome-post-as-my-own-manual-for-managing-this-blog",
    "href": "posts/welcome/index.html#using-the-welcome-post-as-my-own-manual-for-managing-this-blog",
    "title": "Welcome To My Blog",
    "section": "Using the Welcome Post as my own manual for managing this blog",
    "text": "Using the Welcome Post as my own manual for managing this blog"
  },
  {
    "objectID": "posts/ISLP1/index.html",
    "href": "posts/ISLP1/index.html",
    "title": "ISLP Introduction",
    "section": "",
    "text": "I discovered “An Introduction to Statistical Learning” (ISLR) a few years ago, and since then, it’s become my go-to reference for machine learning. A real classic!\nLately, I’ve been itching to dive deeper into the new edition—especially with the updated chapters, Python labs, and all the fresh content.\nBetween the book itself, the video lectures available on edX (and other platforms), and the awesome book club run by the DSLC community, I’ve really immersed myself in this world.\nIt’s been the perfect opportunity to revisit the fundamentals with a fresh, more relaxed mindset. Learning just for the joy of it—rather than to pass a test—is a totally different experience. And honestly, it’s so much more satisfying.\nTo keep a record of this deep dive, I decided to start a blog. The good news? It’s incredibly easy to set up with Quarto, and hosting it on GitHub is a breeze.\nI’m not trying to summarize the book or highlight the key concepts. That’s not the goal. I just want to take the time to explore the parts that speak to me—to reflect, to play, to understand better. Kind of like a personal study journal, but online.\nAll my modest reflections can be found under “ISLP” in the top banner of this website.\n\n\n💻 Data Simulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\n\n# Fixer le seed pour la reproductibilité\nnp.random.seed(42)\n\n# Nombre d'observations par classe\nn = 20\n\n# Génération des données pour la classe 0 (centre = [1, 1])\nX0 = np.random.multivariate_normal(mean=[1, 1], cov=[[1, 0.5], [0.5, 1]], size=n)\ny0 = np.zeros(n)\n\n# Génération des données pour la classe 1 (centre = [3, 3])\nX1 = np.random.multivariate_normal(mean=[3, 3], cov=[[1, 0.5], [0.5, 1]], size=n)\ny1 = np.ones(n)\n\n# Combinaison des deux classes\nX = np.vstack((X0, X1))\ny = np.hstack((y0, y1))\n\n# Mise en DataFrame\ndf = pd.DataFrame(X, columns=['X1', 'X2'])\ndf['Y'] = y.astype(int)\n\n\n\n\n📊 Données simulées par classe\n\n\nCode\nplt.figure(figsize=(6, 5))\nplt.scatter(df[df.Y == 0]['X1'], df[df.Y == 0]['X2'], label='Class 0', alpha=0.7)\nplt.scatter(df[df.Y == 1]['X1'], df[df.Y == 1]['X2'], label='Classe 1', alpha=0.7)\nplt.xlabel(\"X1\")\nplt.ylabel(\"X2\")\nplt.title(\"Simulated Data\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Visualisation 2\nplt.figure(figsize=(6, 5))\nplt.hist(df['X1'], bins=20, color='skyblue')\nplt.title(\"Distribution de X1\")\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "ISLP/index.html",
    "href": "ISLP/index.html",
    "title": "Welcome",
    "section": "",
    "text": "I discovered “An Introduction to Statistical Learning” (ISLR) a few years ago, and since then, it’s become my go-to reference for machine learning. Lately, I’ve been itching to dive deeper into the new edition—especially with the updated chapters, Python labs, and all the fresh content.\nBetween the book itself, the video lectures available on edX (and other platforms), and the awesome book club run by the DSLC community, I’ve really immersed myself in this world.\nIt’s been the perfect opportunity to revisit the fundamentals with a fresh, more relaxed mindset. Learning just for the joy of it—rather than to pass a test—is a totally different experience. And honestly, it’s so much more satisfying.\nTo keep a record of this deep dive, I decided to start a blog. The good news? It’s incredibly easy to set up with Quarto, and hosting it on GitHub is a breeze.\nI’m not trying to summarize the book or highlight the key concepts. That’s not the goal. I just want to take the time to explore the parts that speak to me—to reflect, to play, to understand better. Kind of like a personal study journal, but online.\nISLP newest editation has a chapter on deep learning. I always was curious of deep learning algorithms but never had time to really learnt about them. I am not talking of coding a Neural Network but to underatand what is behind and how some “classical” rules don’t apply\nFreedom or p &gt;&gt; n\nQuand on etudie l’algebre lineaire avant d’apprendre la linear regression, on pense de suite a inversibilite des matrices donc le nombre de parametres doit necessairesment etre plus petit que n. Beacoup de loi statistique depend d’un degree de freedom Comme le Chi2, la loi de student et la loi de Fisher.\nIl est vrai qu’on peut trouver des solutions - Gradient Descent ou utiliser des methodes de regularisations pour les cas ou p &gt; n.\nDans les reseaux neuronaux le nobre de parametres est plus grand que le nombre d’observation. Consider a neural network with two hidden layers: p = 4 input units, 2 units in the frst hidden layer, 3 units in the second hidden layer, and a single output. –&gt; 23 parameters (4+1)2 + (2+1)3 + (3+1)*1\nValidation"
  },
  {
    "objectID": "islp.html",
    "href": "islp.html",
    "title": "Welcome",
    "section": "",
    "text": "I discovered “An Introduction to Statistical Learning” (ISLR) a few years ago, and since then, it’s become my go-to reference for machine learning. Lately, I’ve been itching to dive deeper into the new edition—especially with the updated chapters, Python labs, and all the fresh content.\nBetween the book itself, the video lectures available on edX (and other platforms), and the awesome book club run by the DSLC community, I’ve really immersed myself in this world.\nIt’s been the perfect opportunity to revisit the fundamentals with a fresh, more relaxed mindset. Learning just for the joy of it—rather than to pass a test—is a totally different experience. And honestly, it’s so much more satisfying.\nTo keep a record of this deep dive, I decided to start a blog. The good news? It’s incredibly easy to set up with Quarto, and hosting it on GitHub is a breeze.\nI’m not trying to summarize the book or highlight the key concepts. That’s not the goal. I just want to take the time to explore the parts that speak to me—to reflect, to play, to understand better. Kind of like a personal study journal, but online.\n\n1-Classification\n\nBayses classifiers\n\nLet us focus on understanding the differences between 3 classifiers : Naive Bayses, Linear Discriminant Analysis, and Quadratic Discriminant Analysis.\n\n\nNaive Bayses assume that within the kth class, the p preditors are independent.\nLDA assume that the observations are drawn from a multivariate Gaussian \\(X \\sim \\mathcal{N}(\\mu_k, \\Sigma)\\)\nQDA assume that each class has its own covariance matrix \\(X \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)\\)\n\nPrinciple: We model the distribution of X in each of the classes separately and then we use Bayses theorem to flip things around to obtain Pr(Y|X). By choice, we use the normal (Gaussian) distribution.\n\\[\nPr(Y = k \\mid X = x) = \\frac{\\pi_k f_k(x) }{\\sum_{l=1}^{K} \\pi_l*f_l(x)}\n\\]\nLet us just try on simulated data.\n\nComparation between “manual” and “sklearn” models\n\n\n\n💻 Data Simulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\n\n# Fixer le seed pour la reproductibilité\nnp.random.seed(42)\n\n# 2. Taille des classes\nn_0 = 100  # classe majoritaire\nn_1 = 40   # classe minoritaire\n\n# 3. Paramètres des distributions\n# Classe 0 : petite variance, forte corrélation\nmean_0 = [0, 0]\ncov_0 = [[1, 0.8], [0.8, 1]]\nX0 = np.random.multivariate_normal(mean_0, cov_0, size=n_0)\ny0 = np.zeros(n_0)\n\n# Classe 1 : plus grande variance, faible corrélation\nmean_1 = [3, 3]\ncov_1 = [[2, 0.2], [0.2, 2]]\nX1 = np.random.multivariate_normal(mean_1, cov_1, size=n_1)\ny1 = np.ones(n_1)\n\n# 4. Fusionner les données\nX = np.vstack((X0, X1))\ny = np.hstack((y0, y1))\n\n# Mise en DataFrame\n\n# 5. Création du DataFrame\ndf = pd.DataFrame(X, columns=[\"X1\", \"X2\"])\ndf[\"Y\"] = y.astype(int)\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler(with_mean=True,\n                        with_std=True,\n                        copy=True)\nscaler.fit(df[['X1', 'X2']])\nX_std = scaler.transform(df[['X1', 'X2']])       \n\ndf_scale = pd.DataFrame(\n                 X_std,\n                 columns=['X1', 'X2'])\n\n\n\n\n💻 Manual Classifiers\n# Aperçu des données\n\n\n# 2. Estimer les paramètres nécessaires\n\n# Séparer les classes\ndf_class0 = df[df['Y'] == 0]\nn_0 = len(df_class0)\ndf_class1 = df[df['Y'] == 1]\nn_1 = len(df_class1) \n\n\n\n# Probabilités a priori\nprior_0 = n_0 / len(df)\nprior_1 = n_1 / len(df)\n\n# Moyennes et variances (indépendance des features)\nmu_0 = df_class0[['X1', 'X2']].mean()\nvar_0 = df_class0[['X1', 'X2']].var()\n\nmu_1 = df_class1[['X1', 'X2']].mean()\nvar_1 = df_class1[['X1', 'X2']].var()\n\n\n# Matrices de covariance individuelles\ncov_0 = np.cov(df_class0[['X1', 'X2']].T)\ncov_1 = np.cov(df_class1[['X1', 'X2']].T)\ndet_cov_0 = np.linalg.det(cov_0)\ndet_cov_1 = np.linalg.det(cov_1)\ninv_cov_0 = np.linalg.inv(cov_0)\ninv_cov_1 = np.linalg.inv(cov_1)\n# Matrice de covariance commune (pooled)\npooled_cov = ((n_0 - 1) * cov_0 + (n_1 - 1) * cov_1) / (n_0 + n_1 - 2)\ninv_pooled_cov = np.linalg.inv(pooled_cov)\n\n\n\n\n# Fonction de densité gaussienne\ndef gaussian_pdf(x, mean, var):\n    return (1.0 / np.sqrt(2 * np.pi * var)) * np.exp(- ((x - mean) ** 2) / (2 * var))\n\n# 3. Fonction discriminante LDA\ndef lda_discriminant(x, mu, inv_cov, prior):\n    return x @ inv_cov @ mu - 0.5 * mu.T @ inv_cov @ mu + np.log(prior)\n\n# --- QDA : fonction discriminante ---\ndef qda_discriminant(x, mu, inv_cov, det_cov, prior):\n    return -0.5 * np.log(det_cov) - 0.5 * (x - mu).T @ inv_cov @ (x - mu) + np.log(prior)\n\n    \n# --- Naive Bayes : prédiction manuelle ---\ndef predict_naive_bayes(x):\n    # x est un vecteur numpy, ex : [x1, x2]\n    probs_0 = gaussian_pdf(x, mu_0.values, var_0.values)\n    probs_1 = gaussian_pdf(x, mu_1.values, var_1.values)\n    \n    likelihood_0 = np.prod(probs_0) * prior_0\n    likelihood_1 = np.prod(probs_1) * prior_1\n    \n    return 0 if likelihood_0 &gt; likelihood_1 else 1\n\n# --- LDA : fonction discriminante déjà définie ---\ndef predict_lda(x):\n    score_0 = lda_discriminant(x, mu_0, inv_pooled_cov, prior_0)\n    score_1 = lda_discriminant(x, mu_1, inv_pooled_cov, prior_1)\n    return 0 if score_0 &gt; score_1 else 1\n\n\n# --- QDA : prédiction manuelle ---\ndef predict_qda(x):\n    score_0 = qda_discriminant(x, mu_0, inv_cov_0, det_cov_0, prior_0)\n    score_1 = qda_discriminant(x, mu_1, inv_cov_1, det_cov_1, prior_1)\n    return 0 if score_0 &gt; score_1 else 1\n\n\n\n\n💻 Modelisation\n# Récupération des observations\nX_values = df[['X1', 'X2']].values\nX_values_scale = df_scale[['X1', 'X2']].values\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nimport statsmodels.api as sm\n\nfrom sklearn.metrics import classification_report\n# Dictionnaire des fonctions de prédiction manuelle\nall_models = {\"manual\" :{\n    \"Naive Bayes (manual)\": predict_naive_bayes,\n    \"LDA (manual)\": predict_lda,\n    \"QDA (manual)\": predict_qda},\n    \"non_manual\":{\n     \"Naive Bayes\": GaussianNB(),\n    \"LDA\": LinearDiscriminantAnalysis(),\n    \"QDA\": QuadraticDiscriminantAnalysis(),\n    \"KNN1\" : KNeighborsClassifier(n_neighbors=1),\n    \"KNN2\" : KNeighborsClassifier(n_neighbors=2),\n    \"KNN3\" : KNeighborsClassifier(n_neighbors=3),\n    \"KNN4\" : KNeighborsClassifier(n_neighbors=4),\n    \"KNN5\" : KNeighborsClassifier(n_neighbors=5),\n    \"GLM (Binomial)\": \"glm\"  # on met un identifiant spécial ici}\n}}\n\ndef calcul_accuracy(df):\n# Appliquer chaque prédicteur et stocker l'accuracy\n   all_accuracies = {}\n# Application de chaque modèle\n   for kind, models in all_models.items():\n       for name, model in models.items():\n           if kind == \"manual\":\n               preds = np.array([model(x) for x in X_values])\n           elif model == \"glm\":\n               # Ajout d'une constante pour GLM (intercept)\n               X_glm = sm.add_constant(X_values)\n               glm = sm.GLM(y, X_glm, family=sm.families.Binomial())\n               glm_result = glm.fit()\n               probs = glm_result.predict(X_glm)\n               preds = (probs &gt;= 0.5).astype(int)\n           else:  # sklearn\n               model.fit(X_values, y)\n               preds = model.predict(X_values)\n           acc = classification_report(y, preds, output_dict=True)[\"accuracy\"]\n           all_accuracies[name] = acc\n   return all_accuracies\n    \n\n# Affichage\n#for name, acc in all_accuracies.items():\n#    print(f\"{name}: {acc:.3f}\")\n\n    # Créer un DataFrame pour affichage\naccuracy_df = pd.DataFrame.from_dict(calcul_accuracy(df), orient=\"index\", columns=[\"Accuracy\"])\naccuracy_df.index.name = \"Model\"\n\n\n\n\n📊 Données simulées par classe\n\n\nCode\nplt.figure(figsize=(6, 5))\nplt.scatter(df[df.Y == 0]['X1'], df[df.Y == 0]['X2'], label='Classe 0', alpha=0.7)\nplt.scatter(df[df.Y == 1]['X1'], df[df.Y == 1]['X2'], label='Classe 1', alpha=0.7)\nplt.xlabel(\"X1\")\nplt.ylabel(\"X2\")\nplt.title(\"Simulated Data\")\nplt.legend()\nplt.grid(True)\n#plt.show()\n\n# Visualisation 2\n# Affichage\n#print(accuracy_df)\nfrom IPython.display import display, Markdown\n\n# Arrondir les valeurs à 3 décimales\naccuracy_df_rounded = accuracy_df.copy()\naccuracy_df_rounded[\"Accuracy\"] = accuracy_df_rounded[\"Accuracy\"].apply(lambda x: f\"{x:.3f}\")\n\n# Générer le Markdown avec contrôle CSS\nmd_table = accuracy_df_rounded.to_markdown()\n\n# CSS pour contrôler la largeur des colonnes (via HTML table styling)\nstyle = \"\"\"\n&lt;style&gt;\n&lt;/style&gt;\n\"\"\"\n\n# Affichage avec Markdown + style HTML\ndisplay(Markdown(md_table))\n\n\n\n\n\n\n\nModel\nAccuracy\n\n\n\n\nNaive Bayes (manual)\n0.95\n\n\nLDA (manual)\n0.936\n\n\nQDA (manual)\n0.971\n\n\nNaive Bayes\n0.95\n\n\nLDA\n0.936\n\n\nQDA\n0.971\n\n\nKNN1\n1\n\n\nKNN2\n0.971\n\n\nKNN3\n0.964\n\n\nKNN4\n0.957\n\n\nKNN5\n0.957\n\n\nGLM (Binomial)\n0.943\n\n\n\n\n\n\n\n\n\n\n\nComparation between “manual” and “sklearn” models"
  },
  {
    "objectID": "islp.html#classification",
    "href": "islp.html#classification",
    "title": "Welcome",
    "section": "1 - Classification",
    "text": "1 - Classification"
  },
  {
    "objectID": "islp.html#classification---chap",
    "href": "islp.html#classification---chap",
    "title": "Welcome",
    "section": "Classification - Chap",
    "text": "Classification - Chap\n\nLDA, QDA, Naive Bayses"
  },
  {
    "objectID": "islp.html#classification---chapi",
    "href": "islp.html#classification---chapi",
    "title": "Welcome",
    "section": "Classification - Chapi",
    "text": "Classification - Chapi\n\nLDA, QDA, Naive Bayses"
  },
  {
    "objectID": "islp.html#classification---chapter-3",
    "href": "islp.html#classification---chapter-3",
    "title": "Welcome",
    "section": "Classification - Chapter 3",
    "text": "Classification - Chapter 3\nLDA, QDA, Naive Bayses"
  },
  {
    "objectID": "islp.html#classification-lda-qda-naive-bayses",
    "href": "islp.html#classification-lda-qda-naive-bayses",
    "title": "Welcome",
    "section": "Classification LDA, QDA, Naive Bayses",
    "text": "Classification LDA, QDA, Naive Bayses"
  },
  {
    "objectID": "islp.html#classificationlda-qda-naive-bayses",
    "href": "islp.html#classificationlda-qda-naive-bayses",
    "title": "Welcome",
    "section": "ClassificationLDA, QDA, Naive Bayses",
    "text": "ClassificationLDA, QDA, Naive Bayses"
  },
  {
    "objectID": "islp.html#lda-qda-naive-bayses",
    "href": "islp.html#lda-qda-naive-bayses",
    "title": "Welcome",
    "section": "LDA, QDA, Naive Bayses",
    "text": "LDA, QDA, Naive Bayses\n\nNaive Bayses\nWe model the distribution of X in each of the classes separately and then we use Bayses theorem to flip things around to obtain Pr(Y|X).\n\nNaive Bayses assume the features to be independant.\nLDA assume the covariance to be the same for all the classes.\nQDA assume the covariance to be different.\n\n\n\nDiscriminant Analysis:\nWe model the distribution of X in each of the classes separately and then we use Bayses theorem to flip things around to obtain Pr(Y|X).\nBy choice, we use the normal (Gaussian) distribution\n\\[\nPr(Y = k \\mid X = x) = \\frac{\\pi_k f_k(x) }{\\sum_{l=1}^{K} \\pi_l*f_l(x)}\n\\]\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\n\n# Fixer le seed pour la reproductibilité\nnp.random.seed(42)\n\n# Nombre d'observations par classe\nn = 20\n\n# Génération des données pour la classe 0 (centre = [1, 1])\nX0 = np.random.multivariate_normal(mean=[1, 1], cov=[[1, 0.5], [0.5, 1]], size=n)\ny0 = np.zeros(n)\n\n# Génération des données pour la classe 1 (centre = [3, 3])\nX1 = np.random.multivariate_normal(mean=[3, 3], cov=[[1, 0.5], [0.5, 1]], size=n)\ny1 = np.ones(n)\n\n# Combinaison des deux classes\nX = np.vstack((X0, X1))\ny = np.hstack((y0, y1))\n\n# Mise en DataFrame\ndf = pd.DataFrame(X, columns=['X1', 'X2'])\ndf['Y'] = y.astype(int)\n\n# Visualisation rapide\nplt.figure(figsize=(6, 5))\nplt.scatter(df[df.Y == 0]['X1'], df[df.Y == 0]['X2'], label='Classe 0', alpha=0.7)\nplt.scatter(df[df.Y == 1]['X1'], df[df.Y == 1]['X2'], label='Classe 1', alpha=0.7)\nplt.xlabel(\"X1\")\nplt.ylabel(\"X2\")\nplt.title(\"Données simulées\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Aperçu des données\nprint(df.head())\n\n# 2. Estimer les paramètres nécessaires\n\n# Séparer les classes\ndf_class0 = df[df['Y'] == 0]\ndf_class1 = df[df['Y'] == 1]\n\n# Probabilités a priori\nprior_0 = len(df_class0) / len(df)\nprior_1 = len(df_class1) / len(df)\n\n# Moyennes et variances (indépendance des features)\nmean_0 = df_class0[['X1', 'X2']].mean()\nvar_0 = df_class0[['X1', 'X2']].var()\n\nmean_1 = df_class1[['X1', 'X2']].mean()\nvar_1 = df_class1[['X1', 'X2']].var()\n\n\n# Matrices de covariance individuelles\ncov_0 = np.cov(df_class0[['X1', 'X2']].T)\ncov_1 = np.cov(df_class1[['X1', 'X2']].T)\n\n# Matrice de covariance commune (pooled)\npooled_cov = ((n - 1) * cov_0 + (n - 1) * cov_1) / (2 * n - 2)\ninv_pooled_cov = np.linalg.inv(pooled_cov)\n\n\n# 3. Fonction discriminante LDA\ndef lda_discriminant(x, mu, inv_cov, prior):\n    return x @ inv_cov @ mu - 0.5 * mu.T @ inv_cov @ mu + np.log(prior)\n\n\n# Fonction de densité gaussienne\ndef gaussian_pdf(x, mean, var):\n    return (1.0 / np.sqrt(2 * np.pi * var)) * np.exp(- ((x - mean) ** 2) / (2 * var))\n\n# Prédiction Naive Bayes\ndef predict_naive_bayes(x):\n    prob_0 = prior_0\n    prob_1 = prior_1\n    for i, feature in enumerate(['X1', 'X2']):\n        prob_0 *= gaussian_pdf(x[i], mean_0[feature], var_0[feature])\n        prob_1 *= gaussian_pdf(x[i], mean_1[feature], var_1[feature])\n    return 0 if prob_0 &gt; prob_1 else 1\n\n# Appliquer à toutes les observations\nX_values = df[['X1', 'X2']].values\nnaive_preds = np.array([predict_naive_bayes(x) for x in X_values])\n\n\n# Évaluation\nprint(classification_report(df[\"Y\"], naive_preds))\n\n# 3. Visualisation (optionnelle)\nplt.figure(figsize=(6, 5))\nplt.scatter(df['X1'], df['X2'], c=naive_preds, cmap='bwr', edgecolor='k', alpha=0.6)\nplt.title(\"Prédictions Naive Bayes\")\nplt.xlabel(\"X1\")\nplt.ylabel(\"X2\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n         X1        X2  Y\n0  0.638965  0.500701  0\n1 -0.322430  1.200600  0\n2  1.319851  1.085714  0\n3 -0.751356  0.016079  0\n4  1.135297  1.677857  0\n              precision    recall  f1-score   support\n\n           0       1.00      0.90      0.95        20\n           1       0.91      1.00      0.95        20\n\n    accuracy                           0.95        40\n   macro avg       0.95      0.95      0.95        40\nweighted avg       0.95      0.95      0.95        40\n\n\n\n\n\n\n\n\n\n\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot \\exp\\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right)\n\\]\n\\[\n\\delta_k(x) = x^\\top \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^\\top \\Sigma^{-1} \\mu_k + \\log(\\pi_k)\n\\]"
  },
  {
    "objectID": "islp.html#ff",
    "href": "islp.html#ff",
    "title": "Welcome",
    "section": "",
    "text": "test\nI discovered “An Introduction to Statistical Learning” (ISLR) a few years ago, and since then, it’s become my go-to reference for machine learning. Lately, I’ve been itching to dive deeper into the new edition—especially with the updated chapters, Python labs, and all the fresh content.\nBetween the book itself, the video lectures available on edX (and other platforms), and the awesome book club run by the DSLC community, I’ve really immersed myself in this world.\nIt’s been the perfect opportunity to revisit the fundamentals with a fresh, more relaxed mindset. Learning just for the joy of it—rather than to pass a test—is a totally different experience. And honestly, it’s so much more satisfying.\nTo keep a record of this deep dive, I decided to start a blog. The good news? It’s incredibly easy to set up with Quarto, and hosting it on GitHub is a breeze.\nI’m not trying to summarize the book or highlight the key concepts. That’s not the goal. I just want to take the time to explore the parts that speak to me—to reflect, to play, to understand better. Kind of like a personal study journal, but online."
  },
  {
    "objectID": "islp.html#fff",
    "href": "islp.html#fff",
    "title": "Welcome",
    "section": "",
    "text": "test\nI discovered “An Introduction to Statistical Learning” (ISLR) a few years ago, and since then, it’s become my go-to reference for machine learning. Lately, I’ve been itching to dive deeper into the new edition—especially with the updated chapters, Python labs, and all the fresh content.\nBetween the book itself, the video lectures available on edX (and other platforms), and the awesome book club run by the DSLC community, I’ve really immersed myself in this world.\nIt’s been the perfect opportunity to revisit the fundamentals with a fresh, more relaxed mindset. Learning just for the joy of it—rather than to pass a test—is a totally different experience. And honestly, it’s so much more satisfying.\nTo keep a record of this deep dive, I decided to start a blog. The good news? It’s incredibly easy to set up with Quarto, and hosting it on GitHub is a breeze.\nI’m not trying to summarize the book or highlight the key concepts. That’s not the goal. I just want to take the time to explore the parts that speak to me—to reflect, to play, to understand better. Kind of like a personal study journal, but online."
  },
  {
    "objectID": "islp.html#jjj",
    "href": "islp.html#jjj",
    "title": "Welcome",
    "section": "",
    "text": "test"
  },
  {
    "objectID": "islp.html#résultats-de-lanalyse",
    "href": "islp.html#résultats-de-lanalyse",
    "title": "Welcome",
    "section": "",
    "text": "Du contenu ici…"
  },
  {
    "objectID": "islp.html#test",
    "href": "islp.html#test",
    "title": "Welcome",
    "section": "test",
    "text": "test\n\n1 - Classification\n\nLet us focus on understanding the differences between Naive Bayses, Linear Discriminant Analysis, and Quadratic Discriminant Analysis.\n\n\nNaive Bayses\nPrinciple: We model the distribution of X in each of the classes separately and then we use Bayses theorem to flip things around to obtain Pr(Y|X).\n\nNaive Bayses assume the features to be independant.\nLDA assume the covariance to be the same for all the classes.\nQDA assume the covariance to be different.\n\n\n\n\nDiscriminant Analysis:\nWe model the distribution of X in each of the classes separately and then we use Bayses theorem to flip things around to obtain Pr(Y|X).\nBy choice, we use the normal (Gaussian) distribution\n\\[\nPr(Y = k \\mid X = x) = \\frac{\\pi_k f_k(x) }{\\sum_{l=1}^{K} \\pi_l*f_l(x)}\n\\]\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\n\n# Fixer le seed pour la reproductibilité\nnp.random.seed(42)\n\n# Nombre d'observations par classe\nn = 20\n\n# Génération des données pour la classe 0 (centre = [1, 1])\nX0 = np.random.multivariate_normal(mean=[1, 1], cov=[[1, 0.5], [0.5, 1]], size=n)\ny0 = np.zeros(n)\n\n# Génération des données pour la classe 1 (centre = [3, 3])\nX1 = np.random.multivariate_normal(mean=[3, 3], cov=[[1, 0.5], [0.5, 1]], size=n)\ny1 = np.ones(n)\n\n# Combinaison des deux classes\nX = np.vstack((X0, X1))\ny = np.hstack((y0, y1))\n\n# Mise en DataFrame\ndf = pd.DataFrame(X, columns=['X1', 'X2'])\ndf['Y'] = y.astype(int)\n\n# Visualisation rapide\nplt.figure(figsize=(6, 5))\nplt.scatter(df[df.Y == 0]['X1'], df[df.Y == 0]['X2'], label='Classe 0', alpha=0.7)\nplt.scatter(df[df.Y == 1]['X1'], df[df.Y == 1]['X2'], label='Classe 1', alpha=0.7)\nplt.xlabel(\"X1\")\nplt.ylabel(\"X2\")\nplt.title(\"Données simulées\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Aperçu des données\nprint(df.head())\n\n# 2. Estimer les paramètres nécessaires\n\n# Séparer les classes\ndf_class0 = df[df['Y'] == 0]\ndf_class1 = df[df['Y'] == 1]\n\n# Probabilités a priori\nprior_0 = len(df_class0) / len(df)\nprior_1 = len(df_class1) / len(df)\n\n# Moyennes et variances (indépendance des features)\nmean_0 = df_class0[['X1', 'X2']].mean()\nvar_0 = df_class0[['X1', 'X2']].var()\n\nmean_1 = df_class1[['X1', 'X2']].mean()\nvar_1 = df_class1[['X1', 'X2']].var()\n\n\n# Matrices de covariance individuelles\ncov_0 = np.cov(df_class0[['X1', 'X2']].T)\ncov_1 = np.cov(df_class1[['X1', 'X2']].T)\n\n# Matrice de covariance commune (pooled)\npooled_cov = ((n - 1) * cov_0 + (n - 1) * cov_1) / (2 * n - 2)\ninv_pooled_cov = np.linalg.inv(pooled_cov)\n\n\n# 3. Fonction discriminante LDA\ndef lda_discriminant(x, mu, inv_cov, prior):\n    return x @ inv_cov @ mu - 0.5 * mu.T @ inv_cov @ mu + np.log(prior)\n\n\n# Fonction de densité gaussienne\ndef gaussian_pdf(x, mean, var):\n    return (1.0 / np.sqrt(2 * np.pi * var)) * np.exp(- ((x - mean) ** 2) / (2 * var))\n\n# Prédiction Naive Bayes\ndef predict_naive_bayes(x):\n    prob_0 = prior_0\n    prob_1 = prior_1\n    for i, feature in enumerate(['X1', 'X2']):\n        prob_0 *= gaussian_pdf(x[i], mean_0[feature], var_0[feature])\n        prob_1 *= gaussian_pdf(x[i], mean_1[feature], var_1[feature])\n    return 0 if prob_0 &gt; prob_1 else 1\n\n# Appliquer à toutes les observations\nX_values = df[['X1', 'X2']].values\nnaive_preds = np.array([predict_naive_bayes(x) for x in X_values])\n\n\n# Évaluation\nprint(classification_report(df[\"Y\"], naive_preds))\n\n# 3. Visualisation (optionnelle)\nplt.figure(figsize=(6, 5))\nplt.scatter(df['X1'], df['X2'], c=naive_preds, cmap='bwr', edgecolor='k', alpha=0.6)\nplt.title(\"Prédictions Naive Bayes\")\nplt.xlabel(\"X1\")\nplt.ylabel(\"X2\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n         X1        X2  Y\n0  0.638965  0.500701  0\n1 -0.322430  1.200600  0\n2  1.319851  1.085714  0\n3 -0.751356  0.016079  0\n4  1.135297  1.677857  0\n              precision    recall  f1-score   support\n\n           0       1.00      0.90      0.95        20\n           1       0.91      1.00      0.95        20\n\n    accuracy                           0.95        40\n   macro avg       0.95      0.95      0.95        40\nweighted avg       0.95      0.95      0.95        40\n\n\n\n\n\n\n\n\n\n\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot \\exp\\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right)\n\\]\n\\[\n\\delta_k(x) = x^\\top \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^\\top \\Sigma^{-1} \\mu_k + \\log(\\pi_k)\n\\]"
  },
  {
    "objectID": "islp.html#cross-referencing-a-tip",
    "href": "islp.html#cross-referencing-a-tip",
    "title": "Welcome",
    "section": "Cross-Referencing a Tip",
    "text": "Cross-Referencing a Tip\nAdd an ID starting with #tip- to reference a tip."
  }
]