{
  "hash": "341c758832a7687ad86ac6b9c2e0c145",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"ISLP Introduction\"\ncategories: [learning]\ntoc: true\n---\n\nI discovered ‚ÄúAn Introduction to Statistical Learning‚Äù (ISLR) a few years ago, and since then, it‚Äôs become my go-to reference for machine learning. A real classic!\n\nLately, I‚Äôve been itching to dive deeper into the new edition‚Äîespecially with the updated chapters, Python labs, and all the fresh content.\n\nBetween the book itself, the video lectures available on edX (and other platforms), and the **awesome** book club run by the DSLC community, I‚Äôve really immersed myself in this world.\n\nIt‚Äôs been the perfect opportunity to revisit the fundamentals with a fresh, more relaxed mindset. Learning just for the joy of it‚Äîrather than to pass a test‚Äîis a totally different experience. And honestly, it‚Äôs so much more satisfying.\n\nTo keep a record of this deep dive, I decided to start a blog. The good news? It‚Äôs incredibly easy to set up with Quarto, and hosting it on GitHub is a breeze.\n\nI‚Äôm not trying to summarize the book or highlight the key concepts. That‚Äôs not the goal. I just want to take the time to explore the parts that speak to me‚Äîto reflect, to play, to understand better. Kind of like a personal study journal, but online.\n\nAll my modest reflections can be found under ‚ÄúISLP‚Äù in the top banner of this website.\n\n::: {#7a2d9ee4 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"üíª Data Simulation\"}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\n\n# Fixer le seed pour la reproductibilit√©\nnp.random.seed(42)\n\n# Nombre d'observations par classe\nn = 20\n\n# G√©n√©ration des donn√©es pour la classe 0 (centre = [1, 1])\nX0 = np.random.multivariate_normal(mean=[1, 1], cov=[[1, 0.5], [0.5, 1]], size=n)\ny0 = np.zeros(n)\n\n# G√©n√©ration des donn√©es pour la classe 1 (centre = [3, 3])\nX1 = np.random.multivariate_normal(mean=[3, 3], cov=[[1, 0.5], [0.5, 1]], size=n)\ny1 = np.ones(n)\n\n# Combinaison des deux classes\nX = np.vstack((X0, X1))\ny = np.hstack((y0, y1))\n\n# Mise en DataFrame\ndf = pd.DataFrame(X, columns=['X1', 'X2'])\ndf['Y'] = y.astype(int)\n```\n:::\n\n\n<details>\n<summary>üìä Donn√©es simul√©es par classe</summary>\n\n::: {#553b1ad9 .cell layout-ncol='2' execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\nplt.figure(figsize=(6, 5))\nplt.scatter(df[df.Y == 0]['X1'], df[df.Y == 0]['X2'], label='Class 0', alpha=0.7)\nplt.scatter(df[df.Y == 1]['X1'], df[df.Y == 1]['X2'], label='Classe 1', alpha=0.7)\nplt.xlabel(\"X1\")\nplt.ylabel(\"X2\")\nplt.title(\"Simulated Data\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Visualisation 2\nplt.figure(figsize=(6, 5))\nplt.hist(df['X1'], bins=20, color='skyblue')\nplt.title(\"Distribution de X1\")\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=502 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){width=483 height=431}\n:::\n:::\n\n\n</details>\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}