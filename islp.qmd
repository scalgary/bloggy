---
title: "Welcome"
page-layout: full
toc: true
toc-depth: 4

editor: visual
jupyter: python3
---



I discovered “An Introduction to Statistical Learning” (ISLR) a few years ago, and since then, it’s become my go-to reference for machine learning. Lately, I’ve been itching to dive deeper into the new edition—especially with the updated chapters, Python labs, and all the fresh content.


Between the book itself, the video lectures available on edX (and other platforms), and the awesome book club run by the DSLC community, I’ve really immersed myself in this world.

It’s been the perfect opportunity to revisit the fundamentals with a fresh, more relaxed mindset. Learning just for the joy of it—rather than to pass a test—is a totally different experience. And honestly, it’s so much more satisfying.

To keep a record of this deep dive, I decided to start a blog. The good news? It’s incredibly easy to set up with Quarto, and hosting it on GitHub is a breeze.

I’m not trying to summarize the book or highlight the key concepts. That’s not the goal. I just want to take the time to explore the parts that speak to me—to reflect, to play, to understand better. Kind of like a personal study journal, but online.


#### 1 - Classification

##### LDA, QDA, Naive Bayses

#### Naive Bayses

We model the distribution of X in each of the classes separately and then we use Bayses theorem to flip things around to obtain Pr(Y\|X).

-   Naive Bayses assume the features to be independant.
-   LDA assume the covariance to be the same for all the classes.
-   QDA assume the covariance to be different.

#### Discriminant Analysis:

We model the distribution of X in each of the classes separately and then we use Bayses theorem to flip things around to obtain Pr(Y\|X).

By choice, we use the normal (Gaussian) distribution

$$
Pr(Y = k \mid X = x) = \frac{\pi_k f_k(x) }{\sum_{l=1}^{K} \pi_l*f_l(x)}
$$

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report

# Fixer le seed pour la reproductibilité
np.random.seed(42)

# Nombre d'observations par classe
n = 20

# Génération des données pour la classe 0 (centre = [1, 1])
X0 = np.random.multivariate_normal(mean=[1, 1], cov=[[1, 0.5], [0.5, 1]], size=n)
y0 = np.zeros(n)

# Génération des données pour la classe 1 (centre = [3, 3])
X1 = np.random.multivariate_normal(mean=[3, 3], cov=[[1, 0.5], [0.5, 1]], size=n)
y1 = np.ones(n)

# Combinaison des deux classes
X = np.vstack((X0, X1))
y = np.hstack((y0, y1))

# Mise en DataFrame
df = pd.DataFrame(X, columns=['X1', 'X2'])
df['Y'] = y.astype(int)

# Visualisation rapide
plt.figure(figsize=(6, 5))
plt.scatter(df[df.Y == 0]['X1'], df[df.Y == 0]['X2'], label='Classe 0', alpha=0.7)
plt.scatter(df[df.Y == 1]['X1'], df[df.Y == 1]['X2'], label='Classe 1', alpha=0.7)
plt.xlabel("X1")
plt.ylabel("X2")
plt.title("Données simulées")
plt.legend()
plt.grid(True)
plt.show()

# Aperçu des données
print(df.head())

# 2. Estimer les paramètres nécessaires

# Séparer les classes
df_class0 = df[df['Y'] == 0]
df_class1 = df[df['Y'] == 1]

# Probabilités a priori
prior_0 = len(df_class0) / len(df)
prior_1 = len(df_class1) / len(df)

# Moyennes et variances (indépendance des features)
mean_0 = df_class0[['X1', 'X2']].mean()
var_0 = df_class0[['X1', 'X2']].var()

mean_1 = df_class1[['X1', 'X2']].mean()
var_1 = df_class1[['X1', 'X2']].var()


# Matrices de covariance individuelles
cov_0 = np.cov(df_class0[['X1', 'X2']].T)
cov_1 = np.cov(df_class1[['X1', 'X2']].T)

# Matrice de covariance commune (pooled)
pooled_cov = ((n - 1) * cov_0 + (n - 1) * cov_1) / (2 * n - 2)
inv_pooled_cov = np.linalg.inv(pooled_cov)


# 3. Fonction discriminante LDA
def lda_discriminant(x, mu, inv_cov, prior):
    return x @ inv_cov @ mu - 0.5 * mu.T @ inv_cov @ mu + np.log(prior)


# Fonction de densité gaussienne
def gaussian_pdf(x, mean, var):
    return (1.0 / np.sqrt(2 * np.pi * var)) * np.exp(- ((x - mean) ** 2) / (2 * var))

# Prédiction Naive Bayes
def predict_naive_bayes(x):
    prob_0 = prior_0
    prob_1 = prior_1
    for i, feature in enumerate(['X1', 'X2']):
        prob_0 *= gaussian_pdf(x[i], mean_0[feature], var_0[feature])
        prob_1 *= gaussian_pdf(x[i], mean_1[feature], var_1[feature])
    return 0 if prob_0 > prob_1 else 1

# Appliquer à toutes les observations
X_values = df[['X1', 'X2']].values
naive_preds = np.array([predict_naive_bayes(x) for x in X_values])


# Évaluation
print(classification_report(df["Y"], naive_preds))

# 3. Visualisation (optionnelle)
plt.figure(figsize=(6, 5))
plt.scatter(df['X1'], df['X2'], c=naive_preds, cmap='bwr', edgecolor='k', alpha=0.6)
plt.title("Prédictions Naive Bayes")
plt.xlabel("X1")
plt.ylabel("X2")
plt.grid(True)
plt.show()

```

$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \cdot \exp\left( -\frac{(x - \mu)^2}{2\sigma^2} \right)
$$

$$
\delta_k(x) = x^\top \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^\top \Sigma^{-1} \mu_k + \log(\pi_k)
$$


