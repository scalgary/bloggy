{
  "hash": "8b13073e7bf55f94b6e85da191878305",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Welcome\npage-layout: full\ntoc: true\ntoc-depth: 6\n---\n\nI discovered ‚ÄúAn Introduction to Statistical Learning‚Äù (ISLR) a few years ago, and since then, it‚Äôs become my go-to reference for machine learning. Lately, I‚Äôve been itching to dive deeper into the new edition‚Äîespecially with the updated chapters, Python labs, and all the fresh content.\n\n\nBetween the book itself, the video lectures available on edX (and other platforms), and the awesome book club run by the DSLC community, I‚Äôve really immersed myself in this world.\n\nIt‚Äôs been the perfect opportunity to revisit the fundamentals with a fresh, more relaxed mindset. Learning just for the joy of it‚Äîrather than to pass a test‚Äîis a totally different experience. And honestly, it‚Äôs so much more satisfying.\n\nTo keep a record of this deep dive, I decided to start a blog. The good news? It‚Äôs incredibly easy to set up with Quarto, and hosting it on GitHub is a breeze.\n\nI‚Äôm not trying to summarize the book or highlight the key concepts. That‚Äôs not the goal. I just want to take the time to explore the parts that speak to me‚Äîto reflect, to play, to understand better. Kind of like a personal study journal, but online.\n\n#### 1-Classification\n\n##### Bayses classifiers\n::: {.sandrine}\nLet us focus on understanding the differences between 3 classifiers : **Naive Bayses**, **Linear Discriminant Analysis**, and **Quadratic Discriminant Analysis**.\n:::\n\n-   Naive Bayses assume that within the kth class, the p preditors are **independent**.\n-   LDA assume that the observations are drawn from a multivariate Gaussian $X \\sim \\mathcal{N}(\\mu_k, \\Sigma)$\n-   QDA assume that each class has its own covariance matrix $X \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)$ \n\nPrinciple: We model the distribution of X in each of the classes separately and then we use Bayses theorem to flip things around to obtain Pr(Y|X).\nBy choice, we use the normal (Gaussian) distribution.\n\n\n\n\n\n$$\nPr(Y = k \\mid X = x) = \\frac{\\pi_k f_k(x) }{\\sum_{l=1}^{K} \\pi_l*f_l(x)}\n$$\n\n\nLet us just try on simulated data.\n\n::: {.sandrine}\n###### **Comparation between \"manual\" and \"sklearn\" models** \n:::\n\n::: {#6861e84d .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"üíª Data Simulation\"}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\n\n# Fixer le seed pour la reproductibilit√©\nnp.random.seed(42)\n\n# 2. Taille des classes\nn_0 = 100  # classe majoritaire\nn_1 = 40   # classe minoritaire\n\n# 3. Param√®tres des distributions\n# Classe 0 : petite variance, forte corr√©lation\nmean_0 = [0, 0]\ncov_0 = [[1, 0.8], [0.8, 1]]\nX0 = np.random.multivariate_normal(mean_0, cov_0, size=n_0)\ny0 = np.zeros(n_0)\n\n# Classe 1 : plus grande variance, faible corr√©lation\nmean_1 = [3, 3]\ncov_1 = [[2, 0.2], [0.2, 2]]\nX1 = np.random.multivariate_normal(mean_1, cov_1, size=n_1)\ny1 = np.ones(n_1)\n\n# 4. Fusionner les donn√©es\nX = np.vstack((X0, X1))\ny = np.hstack((y0, y1))\n\n# Mise en DataFrame\n\n# 5. Cr√©ation du DataFrame\ndf = pd.DataFrame(X, columns=[\"X1\", \"X2\"])\ndf[\"Y\"] = y.astype(int)\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler(with_mean=True,\n                        with_std=True,\n                        copy=True)\nscaler.fit(df[['X1', 'X2']])\nX_std = scaler.transform(df[['X1', 'X2']])       \n\ndf_scale = pd.DataFrame(\n                 X_std,\n                 columns=['X1', 'X2'])\n```\n:::\n\n\n::: {#5e08e167 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"üíª Manual Classifiers\"}\n# Aper√ßu des donn√©es\n\n\n# 2. Estimer les param√®tres n√©cessaires\n\n# S√©parer les classes\ndf_class0 = df[df['Y'] == 0]\nn_0 = len(df_class0)\ndf_class1 = df[df['Y'] == 1]\nn_1 = len(df_class1) \n\n\n\n# Probabilit√©s a priori\nprior_0 = n_0 / len(df)\nprior_1 = n_1 / len(df)\n\n# Moyennes et variances (ind√©pendance des features)\nmu_0 = df_class0[['X1', 'X2']].mean()\nvar_0 = df_class0[['X1', 'X2']].var()\n\nmu_1 = df_class1[['X1', 'X2']].mean()\nvar_1 = df_class1[['X1', 'X2']].var()\n\n\n# Matrices de covariance individuelles\ncov_0 = np.cov(df_class0[['X1', 'X2']].T)\ncov_1 = np.cov(df_class1[['X1', 'X2']].T)\ndet_cov_0 = np.linalg.det(cov_0)\ndet_cov_1 = np.linalg.det(cov_1)\ninv_cov_0 = np.linalg.inv(cov_0)\ninv_cov_1 = np.linalg.inv(cov_1)\n# Matrice de covariance commune (pooled)\npooled_cov = ((n_0 - 1) * cov_0 + (n_1 - 1) * cov_1) / (n_0 + n_1 - 2)\ninv_pooled_cov = np.linalg.inv(pooled_cov)\n\n\n\n\n# Fonction de densit√© gaussienne\ndef gaussian_pdf(x, mean, var):\n    return (1.0 / np.sqrt(2 * np.pi * var)) * np.exp(- ((x - mean) ** 2) / (2 * var))\n\n# 3. Fonction discriminante LDA\ndef lda_discriminant(x, mu, inv_cov, prior):\n    return x @ inv_cov @ mu - 0.5 * mu.T @ inv_cov @ mu + np.log(prior)\n\n# --- QDA : fonction discriminante ---\ndef qda_discriminant(x, mu, inv_cov, det_cov, prior):\n    return -0.5 * np.log(det_cov) - 0.5 * (x - mu).T @ inv_cov @ (x - mu) + np.log(prior)\n\n    \n# --- Naive Bayes : pr√©diction manuelle ---\ndef predict_naive_bayes(x):\n    # x est un vecteur numpy, ex : [x1, x2]\n    probs_0 = gaussian_pdf(x, mu_0.values, var_0.values)\n    probs_1 = gaussian_pdf(x, mu_1.values, var_1.values)\n    \n    likelihood_0 = np.prod(probs_0) * prior_0\n    likelihood_1 = np.prod(probs_1) * prior_1\n    \n    return 0 if likelihood_0 > likelihood_1 else 1\n\n# --- LDA : fonction discriminante d√©j√† d√©finie ---\ndef predict_lda(x):\n    score_0 = lda_discriminant(x, mu_0, inv_pooled_cov, prior_0)\n    score_1 = lda_discriminant(x, mu_1, inv_pooled_cov, prior_1)\n    return 0 if score_0 > score_1 else 1\n\n\n# --- QDA : pr√©diction manuelle ---\ndef predict_qda(x):\n    score_0 = qda_discriminant(x, mu_0, inv_cov_0, det_cov_0, prior_0)\n    score_1 = qda_discriminant(x, mu_1, inv_cov_1, det_cov_1, prior_1)\n    return 0 if score_0 > score_1 else 1\n```\n:::\n\n\n::: {#0c1f7937 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"üíª Modelisation\"}\n# R√©cup√©ration des observations\nX_values = df[['X1', 'X2']].values\nX_values_scale = df_scale[['X1', 'X2']].values\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nimport statsmodels.api as sm\n\nfrom sklearn.metrics import classification_report\n# Dictionnaire des fonctions de pr√©diction manuelle\nall_models = {\"manual\" :{\n    \"Naive Bayes (manual)\": predict_naive_bayes,\n    \"LDA (manual)\": predict_lda,\n    \"QDA (manual)\": predict_qda},\n    \"non_manual\":{\n     \"Naive Bayes\": GaussianNB(),\n    \"LDA\": LinearDiscriminantAnalysis(),\n    \"QDA\": QuadraticDiscriminantAnalysis(),\n    \"KNN1\" : KNeighborsClassifier(n_neighbors=1),\n    \"KNN2\" : KNeighborsClassifier(n_neighbors=2),\n    \"KNN3\" : KNeighborsClassifier(n_neighbors=3),\n    \"KNN4\" : KNeighborsClassifier(n_neighbors=4),\n    \"KNN5\" : KNeighborsClassifier(n_neighbors=5),\n    \"GLM (Binomial)\": \"glm\"  # on met un identifiant sp√©cial ici}\n}}\n\ndef calcul_accuracy(df):\n# Appliquer chaque pr√©dicteur et stocker l'accuracy\n   all_accuracies = {}\n# Application de chaque mod√®le\n   for kind, models in all_models.items():\n       for name, model in models.items():\n           if kind == \"manual\":\n               preds = np.array([model(x) for x in X_values])\n           elif model == \"glm\":\n               # Ajout d'une constante pour GLM (intercept)\n               X_glm = sm.add_constant(X_values)\n               glm = sm.GLM(y, X_glm, family=sm.families.Binomial())\n               glm_result = glm.fit()\n               probs = glm_result.predict(X_glm)\n               preds = (probs >= 0.5).astype(int)\n           else:  # sklearn\n               model.fit(X_values, y)\n               preds = model.predict(X_values)\n           acc = classification_report(y, preds, output_dict=True)[\"accuracy\"]\n           all_accuracies[name] = acc\n   return all_accuracies\n    \n\n# Affichage\n#for name, acc in all_accuracies.items():\n#    print(f\"{name}: {acc:.3f}\")\n\n    # Cr√©er un DataFrame pour affichage\naccuracy_df = pd.DataFrame.from_dict(calcul_accuracy(df), orient=\"index\", columns=[\"Accuracy\"])\naccuracy_df.index.name = \"Model\"\n\n```\n:::\n\n\n<details>\n<summary>üìä Donn√©es simul√©es par classe</summary>\n\n::: {#442b51eb .cell layout-ncol='2' execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\nplt.figure(figsize=(6, 5))\nplt.scatter(df[df.Y == 0]['X1'], df[df.Y == 0]['X2'], label='Classe 0', alpha=0.7)\nplt.scatter(df[df.Y == 1]['X1'], df[df.Y == 1]['X2'], label='Classe 1', alpha=0.7)\nplt.xlabel(\"X1\")\nplt.ylabel(\"X2\")\nplt.title(\"Simulated Data\")\nplt.legend()\nplt.grid(True)\n#plt.show()\n\n# Visualisation 2\n# Affichage\n#print(accuracy_df)\nfrom IPython.display import display, Markdown\n\n# Arrondir les valeurs √† 3 d√©cimales\naccuracy_df_rounded = accuracy_df.copy()\naccuracy_df_rounded[\"Accuracy\"] = accuracy_df_rounded[\"Accuracy\"].apply(lambda x: f\"{x:.3f}\")\n\n# G√©n√©rer le Markdown avec contr√¥le CSS\nmd_table = accuracy_df_rounded.to_markdown()\n\n# CSS pour contr√¥ler la largeur des colonnes (via HTML table styling)\nstyle = \"\"\"\n<style>\n</style>\n\"\"\"\n\n# Affichage avec Markdown + style HTML\ndisplay(Markdown(md_table))\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n| Model                |   Accuracy |\n|:---------------------|-----------:|\n| Naive Bayes (manual) |      0.95  |\n| LDA (manual)         |      0.936 |\n| QDA (manual)         |      0.971 |\n| Naive Bayes          |      0.95  |\n| LDA                  |      0.936 |\n| QDA                  |      0.971 |\n| KNN1                 |      1     |\n| KNN2                 |      0.971 |\n| KNN3                 |      0.964 |\n| KNN4                 |      0.957 |\n| KNN5                 |      0.957 |\n| GLM (Binomial)       |      0.943 |\n:::\n\n::: {.cell-output .cell-output-display}\n![](islp_files/figure-html/cell-5-output-2.png){width=513 height=449}\n:::\n:::\n\n\n</details>\n\n\n\n::: {.sandrine}\n###### **Comparation between \"manual\" and \"sklearn\" models** \n:::\n\n",
    "supporting": [
      "islp_files"
    ],
    "filters": [],
    "includes": {}
  }
}