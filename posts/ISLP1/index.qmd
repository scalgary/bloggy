---
title: "ISLP Introduction"
categories: [learning]
toc: true
---

I discovered “An Introduction to Statistical Learning” (ISLR) a few years ago, and since then, it’s become my go-to reference for machine learning. Lately, I’ve been itching to dive deeper into the new edition—especially with the updated chapters, Python labs, and all the fresh content.

Between the book itself, the video lectures available on edX (and other platforms), and the awesome book club run by the DSLC community, I’ve really immersed myself in this world.

It’s been the perfect opportunity to revisit the fundamentals with a fresh, more relaxed mindset. Learning just for the joy of it—rather than to pass a test—is a totally different experience. And honestly, it’s so much more satisfying.

To keep a record of this deep dive, I decided to start a blog. The good news? It’s incredibly easy to set up with Quarto, and hosting it on GitHub is a breeze.

I’m not trying to summarize the book or highlight the key concepts. That’s not the goal. I just want to take the time to explore the parts that speak to me—to reflect, to play, to understand better. Kind of like a personal study journal, but online.

ISLP newest editation has a chapter on deep learning.
I always was curious of deep learning algorithms but never had time to really learnt about them.
I am not talking of coding a Neural Network but to underatand what is behind and how some "classical" rules don't apply

Freedom or p >> n

Quand on etudie l'algebre lineaire avant d'apprendre la linear regression, on pense de suite a inversibilite des matrices donc le nombre de parametres doit necessairesment etre plus petit que n. Beacoup de loi statistique depend d'un degree de freedom Comme le Chi2, la loi de student et la loi de Fisher.

Il est vrai qu'on peut trouver des solutions - Gradient Descent ou utiliser des methodes de regularisations pour les cas ou p > n.

Dans les reseaux neuronaux le nobre de parametres est plus grand que le nombre d'observation.
Consider a neural network with two hidden layers: p = 4 input units,
2 units in the frst hidden layer, 3 units in the second hidden layer,
and a single output. --> 23 parameters
(4+1)*2 + (2+1)*3 + (3+1)*1


Validation


