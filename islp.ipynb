{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Welcome\n",
        "page-layout: full\n",
        "toc: true\n",
        "toc-depth: 6\n",
        "---"
      ],
      "id": "2179bd67"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I discovered “An Introduction to Statistical Learning” (ISLR) a few years ago, and since then, it’s become my go-to reference for machine learning. Lately, I’ve been itching to dive deeper into the new edition—especially with the updated chapters, Python labs, and all the fresh content.\n",
        "\n",
        "\n",
        "Between the book itself, the video lectures available on edX (and other platforms), and the awesome book club run by the DSLC community, I’ve really immersed myself in this world.\n",
        "\n",
        "It’s been the perfect opportunity to revisit the fundamentals with a fresh, more relaxed mindset. Learning just for the joy of it—rather than to pass a test—is a totally different experience. And honestly, it’s so much more satisfying.\n",
        "\n",
        "To keep a record of this deep dive, I decided to start a blog. The good news? It’s incredibly easy to set up with Quarto, and hosting it on GitHub is a breeze.\n",
        "\n",
        "I’m not trying to summarize the book or highlight the key concepts. That’s not the goal. I just want to take the time to explore the parts that speak to me—to reflect, to play, to understand better. Kind of like a personal study journal, but online.\n",
        "\n",
        "#### 1-Classification\n",
        "\n",
        "##### Bayses classifiers\n",
        "::: {.sandrine}\n",
        "Let us focus on understanding the differences between 3 classifiers : **Naive Bayses**, **Linear Discriminant Analysis**, and **Quadratic Discriminant Analysis**.\n",
        ":::\n",
        "\n",
        "-   Naive Bayses assume that within the kth class, the p preditors are **independent**.\n",
        "-   LDA assume that the observations are drawn from a multivariate Gaussian $X \\sim \\mathcal{N}(\\mu_k, \\Sigma)$\n",
        "-   QDA assume that each class has its own covariance matrix $X \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)$ \n",
        "\n",
        "Principle: We model the distribution of X in each of the classes separately and then we use Bayses theorem to flip things around to obtain Pr(Y|X).\n",
        "By choice, we use the normal (Gaussian) distribution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "Pr(Y = k \\mid X = x) = \\frac{\\pi_k f_k(x) }{\\sum_{l=1}^{K} \\pi_l*f_l(x)}\n",
        "$$\n",
        "\n",
        "\n",
        "Let us just try on simulated data.\n",
        "\n",
        "::: {.sandrine}\n",
        "###### **Comparation between \"manual\" and \"sklearn\" models** \n",
        ":::\n"
      ],
      "id": "c9040499"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: \"💻 Data Simulation\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Fixer le seed pour la reproductibilité\n",
        "np.random.seed(42)\n",
        "\n",
        "# 2. Taille des classes\n",
        "n_0 = 100  # classe majoritaire\n",
        "n_1 = 40   # classe minoritaire\n",
        "\n",
        "# 3. Paramètres des distributions\n",
        "# Classe 0 : petite variance, forte corrélation\n",
        "mean_0 = [0, 0]\n",
        "cov_0 = [[1, 0.8], [0.8, 1]]\n",
        "X0 = np.random.multivariate_normal(mean_0, cov_0, size=n_0)\n",
        "y0 = np.zeros(n_0)\n",
        "\n",
        "# Classe 1 : plus grande variance, faible corrélation\n",
        "mean_1 = [3, 3]\n",
        "cov_1 = [[2, 0.2], [0.2, 2]]\n",
        "X1 = np.random.multivariate_normal(mean_1, cov_1, size=n_1)\n",
        "y1 = np.ones(n_1)\n",
        "\n",
        "# 4. Fusionner les données\n",
        "X = np.vstack((X0, X1))\n",
        "y = np.hstack((y0, y1))\n",
        "\n",
        "# Mise en DataFrame\n",
        "\n",
        "# 5. Création du DataFrame\n",
        "df = pd.DataFrame(X, columns=[\"X1\", \"X2\"])\n",
        "df[\"Y\"] = y.astype(int)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler(with_mean=True,\n",
        "                        with_std=True,\n",
        "                        copy=True)\n",
        "scaler.fit(df[['X1', 'X2']])\n",
        "X_std = scaler.transform(df[['X1', 'X2']])       \n",
        "\n",
        "df_scale = pd.DataFrame(\n",
        "                 X_std,\n",
        "                 columns=['X1', 'X2'])\n",
        "df_scale[\"Y\"] = y.astype(int)"
      ],
      "id": "37d28962",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: \"💻 Manual Classifiers\"\n",
        "# Aperçu des données\n",
        "\n",
        "\n",
        "# 2. Estimer les paramètres nécessaires\n",
        "\n",
        "def get_stat(df):\n",
        "   # Séparer les classes\n",
        "    df_class0 = df[df['Y'] == 0]\n",
        "    n_0 = len(df_class0)\n",
        "    df_class1 = df[df['Y'] == 1]\n",
        "    n_1 = len(df_class1) \n",
        "\n",
        "\n",
        "\n",
        "    # Probabilités a priori\n",
        "    prior_0 = n_0 / len(df)\n",
        "    prior_1 = n_1 / len(df)\n",
        "\n",
        "    # Moyennes et variances (indépendance des features)\n",
        "    mu_0 = df_class0[['X1', 'X2']].mean()\n",
        "    var_0 = df_class0[['X1', 'X2']].var()\n",
        "\n",
        "    mu_1 = df_class1[['X1', 'X2']].mean()\n",
        "    var_1 = df_class1[['X1', 'X2']].var()\n",
        "\n",
        "\n",
        "    # Matrices de covariance individuelles\n",
        "    cov_0 = np.cov(df_class0[['X1', 'X2']].T)\n",
        "    cov_1 = np.cov(df_class1[['X1', 'X2']].T)\n",
        "    det_cov_0 = np.linalg.det(cov_0)\n",
        "    det_cov_1 = np.linalg.det(cov_1)\n",
        "    inv_cov_0 = np.linalg.inv(cov_0)\n",
        "    inv_cov_1 = np.linalg.inv(cov_1)\n",
        "    # Matrice de covariance commune (pooled)\n",
        "    pooled_cov = ((n_0 - 1) * cov_0 + (n_1 - 1) * cov_1) / (n_0 + n_1 - 2)\n",
        "    inv_pooled_cov = np.linalg.inv(pooled_cov)\n",
        "    dict_values ={\n",
        "        \"prior_0\": prior_0,\n",
        "        \"prior_1\": prior_1,\n",
        "        \"mu_0\" :mu_0,\n",
        "        \"mu_1\" :mu_1,\n",
        "        \"var_0\" :var_0,\n",
        "        \"var_1\" :var_1,\n",
        "        \"inv_cov_0\": inv_cov_0,\n",
        "        \"inv_cov_1\": inv_cov_1,\n",
        "        \"det_cov_0\" :det_cov_0,\n",
        "        \"det_cov_1\" :det_cov_1,\n",
        "        \"inv_pooled_cov\": inv_pooled_cov\n",
        "\n",
        "\n",
        "    }\n",
        "    return dict_values\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Fonction de densité gaussienne\n",
        "def gaussian_pdf(x, mean, var):\n",
        "    return (1.0 / np.sqrt(2 * np.pi * var)) * np.exp(- ((x - mean) ** 2) / (2 * var))\n",
        "\n",
        "# 3. Fonction discriminante LDA\n",
        "def lda_discriminant(x, mu, inv_cov, prior):\n",
        "    return x @ inv_cov @ mu - 0.5 * mu.T @ inv_cov @ mu + np.log(prior)\n",
        "\n",
        "# --- QDA : fonction discriminante ---\n",
        "def qda_discriminant(x, mu, inv_cov, det_cov, prior):\n",
        "    return -0.5 * np.log(det_cov) - 0.5 * (x - mu).T @ inv_cov @ (x - mu) + np.log(prior)\n",
        "\n",
        "    \n",
        "# --- Naive Bayes : prédiction manuelle ---\n",
        "def predict_naive_bayes(x, stats):\n",
        "    probs_0 = gaussian_pdf(x, stats[\"mu_0\"].values, stats[\"var_0\"].values)\n",
        "    probs_1 = gaussian_pdf(x, stats[\"mu_1\"].values, stats[\"var_1\"].values)\n",
        "    likelihood_0 = np.prod(probs_0) * stats[\"prior_0\"]\n",
        "    likelihood_1 = np.prod(probs_1) * stats[\"prior_1\"]\n",
        "    return 0 if likelihood_0 > likelihood_1 else 1\n",
        "\n",
        "def predict_lda(x, stats):\n",
        "  \n",
        "    score_0 = lda_discriminant(x, stats[\"mu_0\"].values, stats[\"inv_pooled_cov\"], stats[\"prior_0\"])\n",
        "score_1 = lda_discriminant(x, stats[\"mu_1\"].values, stats[\"inv_pooled_cov\"], stats[\"prior_1\"])\n",
        "    return 0 if score_0 > score_1 else 1\n",
        "\n",
        "def predict_qda(x, stats):\n",
        "    score_0 = qda_discriminant(x, stats[\"mu_0\"], stats[\"inv_cov_0\"], stats[\"det_cov_0\"], stats[\"prior_0\"])\n",
        "    score_1 = qda_discriminant(x, stats[\"mu_1\"], stats[\"inv_cov_1\"], stats[\"det_cov_1\"], stats[\"prior_1\"])\n",
        "    return 0 if score_0 > score_1 else 1"
      ],
      "id": "cbaa6564",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: \"💻 Modelisation\"\n",
        "# Récupération des observations\n",
        "X_values = df[['X1', 'X2']].values\n",
        "X_values_scale = df_scale[['X1', 'X2']].values\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import statsmodels.api as sm\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "# Dictionnaire des fonctions de prédiction manuelle\n",
        "def calcul_accuracy(df):\n",
        "    X_values = df[['X1', 'X2']].values\n",
        "    y = df[\"Y\"].values\n",
        "    stats = get_stat(df)\n",
        "\n",
        "    # Re-définir les fonctions manuelles en utilisant les stats locales\n",
        "    manual_models = {\n",
        "        \"Naive Bayes (manual)\": lambda x: predict_naive_bayes(x, stats),\n",
        "        \"LDA (manual)\": lambda x: predict_lda(x, stats),\n",
        "        \"QDA (manual)\": lambda x: predict_qda(x, stats)\n",
        "    }\n",
        "\n",
        "    # Inclure aussi les modèles sklearn\n",
        "    non_manual_models = {\n",
        "        \"Naive Bayes\": GaussianNB(),\n",
        "        \"LDA\": LinearDiscriminantAnalysis(),\n",
        "        \"QDA\": QuadraticDiscriminantAnalysis(),\n",
        "        \"KNN1\": KNeighborsClassifier(n_neighbors=1),\n",
        "        \"KNN2\": KNeighborsClassifier(n_neighbors=2),\n",
        "        \"KNN3\": KNeighborsClassifier(n_neighbors=3),\n",
        "        \"KNN4\": KNeighborsClassifier(n_neighbors=4),\n",
        "        \"KNN5\": KNeighborsClassifier(n_neighbors=5),\n",
        "        \"GLM (Binomial)\": \"glm\"\n",
        "    }\n",
        "\n",
        "    all_accuracies = {}\n",
        "\n",
        "    # Prédictions manuelles\n",
        "    for name, predict_func in manual_models.items():\n",
        "        preds = np.array([predict_func(x) for x in X_values])\n",
        "        acc = classification_report(y, preds, output_dict=True)[\"accuracy\"]\n",
        "        all_accuracies[name] = acc\n",
        "\n",
        "    # Prédictions non manuelles\n",
        "    for name, model in non_manual_models.items():\n",
        "        if model == \"glm\":\n",
        "            X_glm = sm.add_constant(X_values)\n",
        "            glm = sm.GLM(y, X_glm, family=sm.families.Binomial())\n",
        "            glm_result = glm.fit()\n",
        "            probs = glm_result.predict(X_glm)\n",
        "            preds = (probs >= 0.5).astype(int)\n",
        "        else:\n",
        "            model.fit(X_values, y)\n",
        "            preds = model.predict(X_values)\n",
        "\n",
        "        acc = classification_report(y, preds, output_dict=True)[\"accuracy\"]\n",
        "        all_accuracies[name] = acc\n",
        "\n",
        "    return all_accuracies\n",
        "    \n",
        "\n",
        "# Affichage\n",
        "#for name, acc in all_accuracies.items():\n",
        "#    print(f\"{name}: {acc:.3f}\")\n",
        "\n",
        "    # Créer un DataFrame pour affichage\n",
        "accuracy_raw = calcul_accuracy(df)\n",
        "accuracy_scale = calcul_accuracy(df_scale)\n",
        "\n",
        "accuracy_df = pd.DataFrame({\n",
        "    \"accuracy_raw\": pd.Series(accuracy_raw),\n",
        "    \"accuracy_scale\": pd.Series(accuracy_scale)})\n",
        "\n",
        "accuracy_df.index.name = \"Model\"\n"
      ],
      "id": "b1dc8c52",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>📊 Données simulées par classe</summary>\n"
      ],
      "id": "f95befbc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| layout-ncol: 2\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(df[df.Y == 0]['X1'], df[df.Y == 0]['X2'], label='Classe 0', alpha=0.7)\n",
        "plt.scatter(df[df.Y == 1]['X1'], df[df.Y == 1]['X2'], label='Classe 1', alpha=0.7)\n",
        "plt.xlabel(\"X1\")\n",
        "plt.ylabel(\"X2\")\n",
        "plt.title(\"Simulated Data\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "#plt.show()\n",
        "\n",
        "# Visualisation 2\n",
        "# Affichage\n",
        "#print(accuracy_df)\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Arrondir les valeurs à 3 décimales\n",
        "accuracy_df_rounded = accuracy_df.copy()\n",
        "accuracy_df_rounded[\"Accuracy\"] = accuracy_df_rounded[\"Accuracy\"].apply(lambda x: f\"{x:.3f}\")\n",
        "\n",
        "# Générer le Markdown avec contrôle CSS\n",
        "md_table = accuracy_df_rounded.to_markdown()\n",
        "\n",
        "# CSS pour contrôler la largeur des colonnes (via HTML table styling)\n",
        "style = \"\"\"\n",
        "<style>\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "# Affichage avec Markdown + style HTML\n",
        "display(Markdown(md_table))"
      ],
      "id": "1dc1525c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</details>\n",
        "\n",
        "\n",
        "\n",
        "::: {.sandrine}\n",
        "###### **Comparation between \"manual\" and \"sklearn\" models** \n",
        ":::"
      ],
      "id": "f97b0d57"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}